{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 3a\n",
    "\n",
    "## Due: Friday, September 30, 2022 at 5pm (submission via Canvas)\n",
    "\n",
    "\n",
    "* Please submit your assignment (notebooks of parts 3a and 3b + Python modules) as **a single .zip file** using Canvas (Assignments --> Assignment 3). Please put the notebooks for Assignment 3a and 3b as well as the Python modules (files ending with .py) in one folder, which you call ASSIGNMENT_3_FIRSTNAME_LASTNAME. Please zip this folder and upload it as your submission.\n",
    "\n",
    "* Please name your zip file with the following naming convention: ASSIGNMENT_3_FIRSTNAME_LASTNAME.zip\n",
    "\n",
    "**IMPORTANTE NOTE**:\n",
    "* The students who follow the Bachelor version of this course, i.e., the course Introduction to Python for Humanities and Social Sciences (L_AABAALG075) as part of the minor Digital Humanities, do **not have to do Exercises 3 and 4 of Assignment 3b**\n",
    "* The other students, i.e., who follow the Master version of  course, which is Programming in Python for Text Analysis (L_AAMPLIN021), are required to **do Exercises 3 and 4 of Assignment 3b**\n",
    "\n",
    "If you have **questions** about this topic, please contact us **(cltl.python.course@gmail.com)**. Questions and answers will be collected on Piazza, so please check if your question has already been answered first.\n",
    "\n",
    "\n",
    "In this block, we covered a lot of ground:\n",
    "\n",
    "* Chapter 12 - Importing external modules \n",
    "* Chapter 13 - Working with Python scripts\n",
    "* Chapter 14 - Reading and writing text files\n",
    "* Chapter 15 - Off to analyzing text \n",
    "\n",
    "\n",
    "In this assignment, you will first complete a number of small exercises about each chapter to make sure you are familiar with the most important concepts. In the second part of the assignment, you will apply your newly acquired skills to write your very own text processing program (ASSIGNMENT-3b) :-). But don't worry, there will be instructions and hints along the way. \n",
    "\n",
    "\n",
    "**Can I use external modules other than the ones treated so far?**\n",
    "\n",
    "For now, please try to avoid it. All the exercises can be solved with what we have covered in block I, II, and III. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Functions & scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Excercise 1:\n",
    "\n",
    "Define a function called `split_sort_text` which takes one positional parameter called **text** (a string).\n",
    "\n",
    "The function:\n",
    "* splits the string on a space character, i.e., ' '\n",
    "* returns all the unique words in alphabetical order as a list.\n",
    "\n",
    "* Hint 1: There is a specific python container which does not allow for duplicates and simply removes them. Use this one. \n",
    "* Hint 2: There is a function which sorts items in an iterable called 'sorted'. Look at the documentation to see how it is used. \n",
    "* Hint 3: Don't forget to write a docstring. Please make sure that the docstring generally explains with the input is, what the function does, and what the function returns. If you want, but this is not needed to receive full points, you can use [reStructuredText](http://docutils.sourceforge.net/rst.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def split_sort_text(text):\n",
    "    \"\"\"\n",
    "    Splits text on the ' ' (space) character and sorts it afterwards (alphabetically)\n",
    "    :param text: Text to be split and sorted (string)\n",
    "    :return: Sorted list of (unique) strings\n",
    "    \"\"\"\n",
    "    # Splits the text based on the ' ' (space) character\n",
    "    # The result may contain duplicates, given that it's a list\n",
    "    splitted_text_list_unsorted = text.split(' ')\n",
    "\n",
    "    # Convert the list to a string to make sure it doesn't contain any more duplicates\n",
    "    splitted_text_set_unsorted = set(splitted_text_list_unsorted)\n",
    "\n",
    "    # Sort the items in the set (sorted() returns a list which is ordered)\n",
    "    splitted_text_list_sorted = sorted(splitted_text_set_unsorted)\n",
    "    return splitted_text_list_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "['a', 'b', 'c']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test case\n",
    "split_sort_text('c a a b')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Working with external modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 2\n",
    "NLTK offers a way of using WordNet in Python. Do some research (using google, because quite frankly, that's what we do very often) and see if you can find out how to import it. WordNet is a computational lexicon which organizes words according to their senses (collected in synsets). See if you can print all the **synset definitions** of the lemma **dog**.\n",
    "\n",
    "Make sure you have run the following cell to make sure you have installed WordNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/maxfaber/nltk_data...\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# uncomment the following line to download material including WordNet\n",
    "nltk.download('book')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset name: 'dog.n.01'\n",
      "definition: 'a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds'\n",
      "\n",
      "Synset name: 'frump.n.01'\n",
      "definition: 'a dull unattractive unpleasant girl or woman'\n",
      "\n",
      "Synset name: 'dog.n.03'\n",
      "definition: 'informal term for a man'\n",
      "\n",
      "Synset name: 'cad.n.01'\n",
      "definition: 'someone who is morally reprehensible'\n",
      "\n",
      "Synset name: 'frank.n.02'\n",
      "definition: 'a smooth-textured sausage of minced beef or pork usually smoked; often served on a bread roll'\n",
      "\n",
      "Synset name: 'pawl.n.01'\n",
      "definition: 'a hinged catch that fits into a notch of a ratchet to move a wheel forward or prevent it from moving backward'\n",
      "\n",
      "Synset name: 'andiron.n.01'\n",
      "definition: 'metal supports for logs in a fireplace'\n",
      "\n",
      "Synset name: 'chase.v.01'\n",
      "definition: 'go after with the intent to catch'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for synset in nltk.corpus.wordnet.synsets('dog'):\n",
    "    print(f'Synset name: \\'{synset.name()}\\'')\n",
    "    print(f'Definition: \\'{synset.definition()}\\'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Working with python scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise  3\n",
    "\n",
    "#### a.) Define a function called `count`, which determines how often each word occurs in a string. Do not use NLTK just yet. Find a way to test it. \n",
    "\n",
    "* Write a helper-function called `preprocess`, which removes the punctuation specified by the user, and returns the same string without the unwanted characters. You call the function `preprocess` inside the `count` function.\n",
    "\n",
    "* Remember that there are string methods that you can use to get rid of unwanted characters. Test the `preprocess` function using the following string `'this is a (tricky) test'`.\n",
    "\n",
    "* Remember how we used dictionaries to count words? If not, have a look at Chapter 10 - Dictionaries. \n",
    "\n",
    "* make sure you split the string on a space character ' '. You loop over the list to count the words.\n",
    "\n",
    "* Test your function using an example string, which will tell you whether it fulfills the requirements (remove punctuation, split, count). You will get a point for good testing.\n",
    "\n",
    "#### b.) Create a python script \n",
    "\n",
    "Use your editor to create a Python script called **count_words.py**. Place the function definition of the **count** function in **count_words.py**. Also put a function call of the **count** function in this file to test it. Place your helper function definition, i.e., **preprocess**, in a separate script called **utils_3a.py**. Import your helper function **preprocess** into count_words.py. Test whether everything works as expected by calling the script count_words.py from the terminal.\n",
    "\n",
    "The function **preprocess** preprocesses the text by removing characters that are unwanted by the user. **preprocess** is called within the **count** function and hence builds upon the output from the preprocess function and creates a dictionary in which the key is a word and the value is the frequency of the word.\n",
    "\n",
    "**Please submit these scripts together with the other notebooks**.\n",
    "\n",
    "Don't forget to add docstrings to your functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before preprocessing: 'this is a (tricky) test'\n",
      "After preprocessing: 'this is a tricky test'\n"
     ]
    }
   ],
   "source": [
    "# Feel free to use this cell to try out your code.\n",
    "from utils_3a import preprocess\n",
    "\n",
    "text = 'this is a (tricky) test'\n",
    "preprocessed_text = preprocess(text)\n",
    "print(f'Before preprocessing: \\'{text}\\'')\n",
    "print(f'After preprocessing: \\'{preprocessed_text}\\'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dealing with text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 4\n",
    "\n",
    "**Playing with lyrics**\n",
    "\n",
    "a.) Write a function called `load_text`, which opens and reads a file and returns the text in the file. It should have the file path as a parameter. Test it by loading this file: ../Data/lyrics/walrus.txt\n",
    "\n",
    "* Hint: remember it is best practice to use a context manager\n",
    "* Hint: **FileNotFoundError**: This means that the path you provide does not lead to an existing file on your computer. Please carefully study Chapter 14. Please determine where the notebook or Python module that you are working with is located on your computer. Try to determine where Python is looking if you provide a path such as “../Data/lyrics/walrus.txt”. Try to go from your notebook to the location on your computer where Python is trying to find the file. One tip: if you did not store the Assignments notebooks 3a and 3b in the folder “Assignments”, you would get this error.\n",
    "\n",
    "b.) Write a function called `replace_walrus`, which takes lyrics as input and replaces every instance of 'walrus' by 'hippo' (make sure to account for upper and lower case - it is fine to transform everything to lower case). The function should write the new version of the song to a file called 'walrus_hippo.txt and stored in ../Data/lyrics. \n",
    "\n",
    "Don't forget to add docstrings to your functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Definition of the directory where the walrus lyrics is stored and the new lyrics will be stored\n",
    "lyrics_dir = '../Data/lyrics'\n",
    "\n",
    "def load_text(file_path):\n",
    "    \"\"\"\n",
    "    Opens and reads text from a file\n",
    "    :param file_path: Path of the file to read from\n",
    "    :return: The contents of the file (string)\n",
    "    \"\"\"\n",
    "    with open(file_path, mode='r', encoding='utf-8') as read_file:\n",
    "        return read_file.read()\n",
    "\n",
    "def replace_walrus(lyrics):\n",
    "    \"\"\"\n",
    "    Replaces all occurrences of the word 'walrus' into 'hippo' and converts the lyrics into lowercase text\n",
    "    :param lyrics: The lyrics to modify\n",
    "    :return: The modified lyrics\n",
    "    \"\"\"\n",
    "    lyrics_lowercase = lyrics.lower()\n",
    "    lyrics_hippo = lyrics_lowercase.replace('walrus', 'hippo')\n",
    "    # Construct the path to write the lyrics to\n",
    "    output_path = os.path.join(lyrics_dir, 'walrus_hippo.txt')\n",
    "    with open(output_path, mode='w', encoding='utf-8') as output_file:\n",
    "        output_file.write(lyrics_hippo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original lyrics:\n",
      "\"I Am The Walrus\"\n",
      "(\"Magical Mystery Tour\" Version)\n",
      "\n",
      "I am he\n",
      "As you are he\n",
      "As you are me\n",
      "And we are all together\n",
      "\n",
      "See how they run\n",
      "Like pigs from a gun\n",
      "See how they fly\n",
      "I'm crying\n",
      "\n",
      "Sitting on a cornflake\n",
      "Waiting for the van to come\n",
      "Corporation tee shirt\n",
      "Stupid bloody Tuesday\n",
      "Man, you been a naughty boy\n",
      "You let your face grow long\n",
      "\n",
      "I am the eggman (Ooh)\n",
      "They are the eggmen, (Ooh)\n",
      "I am the walrus\n",
      "Goo goo g' joob\n",
      "\n",
      "Mister city p'liceman sitting pretty\n",
      "Little p'licemen in a row\n",
      "See how they fly\n",
      "Like Lucy in the sky\n",
      "See how they run\n",
      "I'm crying\n",
      "I'm crying, I'm crying, I'm crying\n",
      "\n",
      "Yellow matter custard\n",
      "Dripping from a dead dog's eye\n",
      "Crabalocker fishwife pornographic priestess\n",
      "Boy you been a naughty girl\n",
      "You let your knickers down\n",
      "\n",
      "I am the eggman (Ooh)\n",
      "They are the eggmen (Ooh)\n",
      "I am the walrus\n",
      "Goo goo g' joob\n",
      "\n",
      "Sitting in an English\n",
      "Garden waiting for the sun\n",
      "If the sun don't come\n",
      "You get a tan from standing in the English rain\n",
      "\n",
      "I am the eggman\n",
      "They are the eggmen\n",
      "I am the walrus\n",
      "Goo goo g' joob g' goo goo g' joob\n",
      "\n",
      "Expert texpert choking smokers\n",
      "Don't you think the joker laughs at you?\n",
      "See how they smile\n",
      "Like pigs in a sty, see how they snied\n",
      "I'm crying\n",
      "\n",
      "Semolina pilchards\n",
      "Climbing up the Eiffel Tower\n",
      "Element'ry penguin singing Hare Krishna\n",
      "Man, you should have seen them kicking Edgar Allan Poe\n",
      "\n",
      "I am the eggman (Ooh)\n",
      "They are the eggmen (Ooh)\n",
      "I am the walrus\n",
      "Goo goo g' joob\n",
      "Goo goo g' joob\n",
      "G' goo goo g' joob\n",
      "Goo goo g' joob, goo goo g' goo g' goo goo g' joob joob\n",
      "Joob joob...\n",
      "\n",
      "\n",
      "Modified lyrics:\n",
      "\"i am the hippo\"\n",
      "(\"magical mystery tour\" version)\n",
      "\n",
      "i am he\n",
      "as you are he\n",
      "as you are me\n",
      "and we are all together\n",
      "\n",
      "see how they run\n",
      "like pigs from a gun\n",
      "see how they fly\n",
      "i'm crying\n",
      "\n",
      "sitting on a cornflake\n",
      "waiting for the van to come\n",
      "corporation tee shirt\n",
      "stupid bloody tuesday\n",
      "man, you been a naughty boy\n",
      "you let your face grow long\n",
      "\n",
      "i am the eggman (ooh)\n",
      "they are the eggmen, (ooh)\n",
      "i am the hippo\n",
      "goo goo g' joob\n",
      "\n",
      "mister city p'liceman sitting pretty\n",
      "little p'licemen in a row\n",
      "see how they fly\n",
      "like lucy in the sky\n",
      "see how they run\n",
      "i'm crying\n",
      "i'm crying, i'm crying, i'm crying\n",
      "\n",
      "yellow matter custard\n",
      "dripping from a dead dog's eye\n",
      "crabalocker fishwife pornographic priestess\n",
      "boy you been a naughty girl\n",
      "you let your knickers down\n",
      "\n",
      "i am the eggman (ooh)\n",
      "they are the eggmen (ooh)\n",
      "i am the hippo\n",
      "goo goo g' joob\n",
      "\n",
      "sitting in an english\n",
      "garden waiting for the sun\n",
      "if the sun don't come\n",
      "you get a tan from standing in the english rain\n",
      "\n",
      "i am the eggman\n",
      "they are the eggmen\n",
      "i am the hippo\n",
      "goo goo g' joob g' goo goo g' joob\n",
      "\n",
      "expert texpert choking smokers\n",
      "don't you think the joker laughs at you?\n",
      "see how they smile\n",
      "like pigs in a sty, see how they snied\n",
      "i'm crying\n",
      "\n",
      "semolina pilchards\n",
      "climbing up the eiffel tower\n",
      "element'ry penguin singing hare krishna\n",
      "man, you should have seen them kicking edgar allan poe\n",
      "\n",
      "i am the eggman (ooh)\n",
      "they are the eggmen (ooh)\n",
      "i am the hippo\n",
      "goo goo g' joob\n",
      "goo goo g' joob\n",
      "g' goo goo g' joob\n",
      "goo goo g' joob, goo goo g' goo g' goo goo g' joob joob\n",
      "joob joob...\n"
     ]
    }
   ],
   "source": [
    "# Construct the path of the lyrics\n",
    "path_walrus = os.path.join(lyrics_dir, 'walrus.txt')\n",
    "lyrics = load_text(path_walrus)\n",
    "print(f'Original lyrics:\\n{lyrics}\\n\\n')\n",
    "\n",
    "# Modifies the lyrics and saves it to a file\n",
    "replace_walrus(lyrics)\n",
    "# Construct path to read the new lyrics from\n",
    "output_path_walrus_hippo = os.path.join(lyrics_dir, 'walrus_hippo.txt')\n",
    "with open(output_path_walrus_hippo, mode='r', encoding='utf-8') as read_file:\n",
    "    hippo_lyrics = read_file.read()\n",
    "    print(f'Modified lyrics:\\n{hippo_lyrics}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Analyzing text with nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 5\n",
    "\n",
    "**Building a simple NLP pipeline**\n",
    "\n",
    "For this exercise, you will need NLTK. Don't forget to import it. \n",
    "\n",
    "Write a function called `tag_text`, which takes raw text as input and returns the tagged text. To do this, make sure you follow the steps below:\n",
    "\n",
    "* Tokenize the text. \n",
    "\n",
    "* Perform part-of-speech tagging on the list of tokens. \n",
    "\n",
    "* Return the tagged text\n",
    "\n",
    "\n",
    "Then test your function using the text snipped below (`test_text`) as input.\n",
    "\n",
    "Please note that the tags may not be correct and that this is not a mistake on your end, but simply NLP tools not being perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_text = \"\"\"Shall I compare thee to a summer's day?\n",
    "Thou art more lovely and more temperate:\n",
    "Rough winds do shake the darling buds of May,\n",
    "And summer's lease hath all too short a date:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def tag_text(raw_text):\n",
    "    \"\"\"\n",
    "    Converts a piece of text to tokens and tags these tokens afterwards\n",
    "    :param raw_text: The text to tokenize and tag\n",
    "    :return: List of tuples containing the tokens in the first element and the corresponding tag in the second element\n",
    "    \"\"\"\n",
    "    tokens = nltk.wordpunct_tokenize(raw_text) # Tokenize the text\n",
    "    tagged_tokens = nltk.pos_tag(tokens) # Tag the tokens of the text\n",
    "    return tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Shall', 'I', 'compare', 'thee', 'to', 'a', 'summer', \"'\", 's', 'day', '?', 'Thou', 'art', 'more', 'lovely', 'and', 'more', 'temperate', ':', 'Rough', 'winds', 'do', 'shake', 'the', 'darling', 'buds', 'of', 'May', ',', 'And', 'summer', \"'\", 's', 'lease', 'hath', 'all', 'too', 'short', 'a', 'date', ':']\n",
      "\n",
      "[('Shall', 'NN'), ('I', 'PRP'), ('compare', 'VBP'), ('thee', 'JJ'), ('to', 'TO'), ('a', 'DT'), ('summer', 'NN'), (\"'\", 'POS'), ('s', 'NN'), ('day', 'NN'), ('?', '.'), ('Thou', 'NNP'), ('art', 'RB'), ('more', 'RBR'), ('lovely', 'RB'), ('and', 'CC'), ('more', 'JJR'), ('temperate', 'NN'), (':', ':'), ('Rough', 'NNP'), ('winds', 'NNS'), ('do', 'VBP'), ('shake', 'VB'), ('the', 'DT'), ('darling', 'VBG'), ('buds', 'NNS'), ('of', 'IN'), ('May', 'NNP'), (',', ','), ('And', 'CC'), ('summer', 'NN'), (\"'\", \"''\"), ('s', 'JJ'), ('lease', 'NN'), ('hath', 'NN'), ('all', 'DT'), ('too', 'RB'), ('short', 'JJ'), ('a', 'DT'), ('date', 'NN'), (':', ':')]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/maxfaber/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger') # Download required resource\n",
    "tagged_text = tag_text(test_text)\n",
    "print(tagged_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Python knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 6\n",
    "\n",
    "6.a) Explain in your own words the difference between the global and the local scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[answer]\n",
    "Every variable or function that you define without any indentation is defined in the global scope. These definitions can be accessed from anywhere in the program, from the same scope, within functions, loops etc.\n",
    "\n",
    "Defining a variable with indentation (in a function for example) on the other hand, is what is called a definition in the local scope. The definition of this variable can only be accessed from the same scope (the current indent) and the child scopes of the current scope (indents to the right).\n",
    "\n",
    "When a variable is defined in the global scope and also in local scope with the same name, then the value of the variable is overwritten in the local scope, but is restored when the program exits the local scope nd returns to the global scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "6.b) What is the difference between the modes 'w' and 'a' when opening a file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[answer]\n",
    "Both modes share the feature of opening a file and support writing to it. If the file doesn't exist, it will be created. However, they differ in their starting points:\n",
    "\n",
    "- The 'w' mode truncates the file first, that is, if there's any contents in the file already, these contents will first be erased.\n",
    "- The 'a' mode on the other hand keeps the file as it was and when you start writing data to that file, it will be appended to the original contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}